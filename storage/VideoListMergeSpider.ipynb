{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import requests\n",
    "import pandas as pd\n",
    "import sys\n",
    "from units import download_by_addr, getConfig\n",
    "from loguru import logger\n",
    "\n",
    "# 配置信息\n",
    "start = int(getConfig(\"gnn_spider\", \"start\"))\n",
    "end = int(getConfig(\"gnn_spider\", \"end\"))\n",
    "keyword = getConfig(\"gnn_spider\", \"keyword\")\n",
    "token = getConfig(\"gnn_spider\", \"token\")\n",
    "record_add='./backup/total_info_record.xlsx'\n",
    "\n",
    "\n",
    "payload = {}\n",
    "headers = {\"User-Agent\": \"Apifox/1.0.0 (https://www.apifox.cn)\"}\n",
    "total_video_list = []\n",
    "failed_target = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.add(\n",
    "    \"log/VideoInfo.log\", filter=lambda record: record[\"extra\"][\"name\"] == \"VideoInfo\"\n",
    ")\n",
    "logger.add(\n",
    "    \"log/VideoDownload.log\",\n",
    "    filter=lambda record: record[\"extra\"][\"name\"] == \"VideoDownload\",\n",
    ")\n",
    "logger.add(\n",
    "    \"log/VideoInspect.log\",\n",
    "    filter=lambda record: record[\"extra\"][\"name\"] == \"VideoInspect\",\n",
    ")\n",
    "# logger.add(\"log/VideoInspectRepeat.log\", filter=lambda record: record[\"extra\"][\"name\"] == \"VideoInspectRepeat\")\n",
    "\n",
    "logger_info = logger.bind(name=\"VideoInfo\")\n",
    "logger_download = logger.bind(name=\"VideoDownload\")\n",
    "logger_inspect = logger.bind(name=\"VideoInspect\")\n",
    "# logger_inspect_repeat = logger.bind(name=\"VideoInspectRepeat\")\n",
    "\n",
    "\n",
    "## 爬取页号\n",
    "for page in range(start, end):\n",
    "    url = f\"http://ttt.258data.com/dy/search/video/app/v3?keyword={keyword}&page={page}&token={token}\"\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "    scrapy_vedio_list = response.json()\n",
    "    if scrapy_vedio_list[\"code\"] == 0:\n",
    "        failed_target = 0\n",
    "        logger_info.info(\"Get Info Success\")  ## 请求返回成功了\n",
    "        total_video_list.extend(scrapy_vedio_list[\"data\"][\"aweme_msg_list\"])\n",
    "    else:  # 请求失败 返回页号\n",
    "        for i in range(3):\n",
    "            response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "            scrapy_vedio_list = response.json()\n",
    "            if scrapy_vedio_list[\"code\"] == 0:\n",
    "                failed_target = 0\n",
    "                logger_info(\"Failed Retry, Success\")  ## 请求返回成功了\n",
    "                total_video_list.extend(scrapy_vedio_list[\"data\"][\"aweme_msg_list\"])\n",
    "                continue\n",
    "        if scrapy_vedio_list[\"code\"] != 0:\n",
    "            failed_target += 1\n",
    "            logger_info.debug(\n",
    "                f\"Page {page} is scrapy failed , output : {scrapy_vedio_list['msg']}\"\n",
    "            )\n",
    "            if failed_target == 3:\n",
    "                logger_info.error(f\"Page {page} is scrapy breaking\")\n",
    "                if len(total_video_list) == 0:\n",
    "                    logger_info.critical(f\"Interface error, no data obtained\")\n",
    "                    sys.exit()\n",
    "                break\n",
    "    ### 部分下载\n",
    "    if scrapy_vedio_list[\"code\"] == 0:\n",
    "        pd.DataFrame(scrapy_vedio_list[\"data\"][\"aweme_msg_list\"]).apply(\n",
    "            lambda x: download_by_addr(\n",
    "                \"./video/\" + str(x.aweme_id), x.play_addr, logger_download\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        logger_info.info(\"Batch Video Download Over\")  ## 下载成功了\n",
    "\n",
    "\n",
    "## 存储爬取信息\n",
    "info = pd.DataFrame(total_video_list).drop_duplicates(subset=[\"aweme_id\"], keep=\"first\")\n",
    "info.to_excel(\"info/search_运动 vlog.xlsx\", encoding=\"utf_8_sig\", index=False)\n",
    "logger_info.success(\n",
    "    f\"Successfully stored crawl information, {len(info)} items get.\"\n",
    ")  ## 爬取信息存储成功了\n",
    "\n",
    "\n",
    "## 检查一次\n",
    "logger_info.info(\n",
    "    f\"Download Inspect\"\n",
    ")  ## 爬取信息存储成功了\n",
    "\n",
    "info.apply(\n",
    "    lambda x: download_by_addr(\n",
    "        \"./video/\" + str(x.aweme_id), x.play_addr, logger_inspect\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrapy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Mar 29 2022, 02:18:16) \n[GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad7a1fbfe381e46b0f3b9a820daa81600bcc600b342b498233523bb6680d89ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
